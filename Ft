# fine_tune_pipeline.py
from datasets import Dataset
import torch
from transformers import AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model

class ProtocolFineTuner:
    def __init__(self, base_model="microsoft/phi-2"):
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
    def prepare_dataset(self, protocols_dir):
        """Convert protocols to instruction format"""
        dataset = []
        for file in Path(protocols_dir).glob("*.md"):
            content = file.read_text()
            # Format as instruction-response pairs
            dataset.append({
                "instruction": "Based on the protocols, provide expert guidance on: ",
                "input": content[:500],  # First 500 chars as context
                "output": self.generate_expected_output(content)
            })
        return Dataset.from_list(dataset)
    
    def train_lora(self, dataset, output_dir="./fine_tuned"):
        """LoRA fine-tuning for efficiency"""
        model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # LoRA configuration
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.1,
            bias="none"
        )
        
        model = get_peft_model(model, lora_config)
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=50,
            max_steps=500,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            save_strategy="steps",
            save_steps=100
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=self.data_collator
        )
        
        trainer.train()
        model.save_pretrained(output_dir)
